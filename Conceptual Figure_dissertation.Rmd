---
title: "Dissertation Mock up figures"
author: "Michelle DePrenger-Levin"
date: "November 7, 2019"
output: html_document
---

```{r}
library(raster)
library(gstat)
library(virtualspecies)
library(DiagrammeR)

# install.packages("BiocManager")
# BiocManager::install("Rgraphviz")
# 
# library(Graphviz)

library(diagram)

```

# Spatial modeling: forecasting and descriptive    
gold standard    
when falls apart for rare plants    


<http://santiago.begueria.es/2010/10/generating-spatially-correlated-random-fields-with-r/>
```{r}
xy <- expand.grid(1:100, 1:100)
names(xy) <-  c('x','y')
# where formula defines the dependent variable (z) as a linear model of independent variables. For ordinary and simple kriging we can use the formula z~1; for simple kriging it is necessary to define a beta parameter too (see below); for universal kriging, if z is linearly dependent on x and y use the formula z~x+y. We are using simple kriging here.
# locations define the data coordinates, e.g. ~x+y in our case here. 
# dummy is a logical value, and it needs to be TRUE for unconditional simulation. 
# beta is used only for simple kriging, and is a vector with the trend coefficients (including an intercept); 
# if no independent variables are defined the model only contains an intercept, i.e. the simple kriging mean. model defines the variogram model, as defined by a call to vgm. vgm allows defining the (partial) sill, range and nugget paramaters, as well as the variogram model type (e.g. exponential, gaussian, spherical, etc). Anisotropy can also be used. 
# nmax defines the number of nearest observations that should be used for a kriging prediction or simulation.

g.dummy <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=1, model=vgm(psill=0.025, range=5, model='Exp'), nmax=20)

yy <- predict(g.dummy, newdata=xy, nsim=4)
gridded(yy) = ~x+y
spplot(obj=yy[1])

spplot(obj=yy[2])

```


```{r}
plot(yy[1])

g.dummy2 <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=3, model=vgm(psill=0.025, range=20, model='Exp'), nmax=20)

yy2 <- predict(g.dummy2, newdata=xy, nsim=4)
gridded(yy2) = ~x+y

g.dummy3 <- gstat(formula=z~1+x+y, locations=~x+y, dummy=T, beta=c(1,0.01,0.005), 
                  model=vgm(psill=0.025, range=50, model='Exp'), nmax=20)

yy3 <- predict(g.dummy3, newdata=xy, nsim=4)
gridded(yy3) = ~x+y

spplot(yy[1])
spplot(yy2[1])
spplot(yy3[1])
```


```{r}
# Different sample sizes
simsp1 <- data.frame(x = runif(10, 1, 99), y = runif(10, 1,99) )
simsp10 <- data.frame(x = runif(100, 1, 99), y = runif(100, 1,99) )



plot(yy[2])
points(simsp1, col = "black", pch = 20)


plot(yy3[3])
points(simsp1, col = "black", pch = 20)


# make points in relation to each layer? 
yy[1]@data$sim1
```








# PVA temporal modeling demograpahics    

DiagrammeR
```{r}
# define nodes dataframe
nodes <- create_node_df(n = 8, 
                        type = "lower",
                        style = "filled",
                        color = "teal", 
                        shape = "circle", 
                        data = c(3.5, 2.6, 9.4, 2.7))

# define edges dataframe
edges <- create_edge_df(from = c(1, 2, 3, 3),
                        to = c(2, 4, 4, 2))

# create graph
my_graph <- create_graph(nodes_df = nodes, edges_df = edges)

# print graph
render_graph(my_graph)


```

diagram   
<https://cran.r-project.org/web/packages/diagram/vignettes/diagram.pdf>    
```{r}
m <- matrix(nrow=8, ncol = 8, byrow = TRUE, data = c(
  # P  f  s  g  dd  sz  c  m
   0, 0, 0, 0, 0, 0, 0, 0,
   1, 0, 0, 0, 0, 0, 0, 0,
   2, 0, 0, 0, 0, 0, 0, 0,
   3, 0, 0, 0, 0, 0, 0, 0,
   4, 0, 0, 0, 0, 0, 0, 0,
   0, 5, 6, 7, 0, 0, 0, 0,
   0, 8, 9,10, 0, 0, 0, 0,
   0,11,12,13, 0, 0, 0, 0
   )) 
names <- c("Population trends", "fecundity", "survival", "growth", "density dependence",
           "size", "climate", "management")

# m[2,1] <- m[3,1] <- m[4,1] <- m[5,1] <- m[6,2] <- m[6,3] <- m[6,4] <- 
#   m[7,2] <- m[7,3] <- m[7,4] <- 
#   m[8,2] <- m[8,3] <- m[8,4] <- "lm" 
plotmat(m, pos = c(1, 4, 3), curve = 0, name = names, 
        box.type = "square", cex.txt = 0.5, box.prop = 0.3,
        arr.type = "triangle", arr.pos = 0.5, prefix = "m")


```



MatrixSplit: Divided means A broken into process-based submatrices U (survival), F (sexual reproduction), and C (clonal reproduction)
```{r}

library(popdemo)

load("C:/Users/deprengm/OneDrive - Denver Botanic Gardens/P drive/My Documents/UCDenver_phd/Dissertation/PVA/Paniw etal/patterns_temporal_autocorrelation-master/COMPADRE_v.4.0.0.RData")

# example from Wall et al. 2012 
table(compadre$metadata$SpeciesAccepted) # how many matrixes there are A
compadre$mat[compadre$metadata$SpeciesAccepted == "Astragalus michauxii"]
compadre$metadata[compadre$metadata$SpeciesAccepted == "Astragalus michauxii",]

```


Get the A matrices   
```{r}
AsMi_A <- lapply(compadre$mat[compadre$metadata$SpeciesAccepted == "Astragalus michauxii"], '[[', 1)


```














Population viability modeling: forecasting and descriptive
gold standard
when falls apart for rare plants




Example using PVA for predicting impact of seed harvest

# Zero-inflated data    
When more zeros than what will fit a standard distribution     
Zero-inflated Poisson regression     
<https://fukamilab.github.io/BIO202/04-C-zero-data.html>     
Two-part models wher ethe count process cannot produce zeros (truncated) vs. mixture models which distriguish between true and false zeros (at least statistically)     
set up glm with random effect x|g where g is the grouping factor    

     1. 1|group for random intercept     
     2. time|group for random variation in slopes through time across groups   
     3. 1|site/group for nested random effects group within site    
     4. (1|group) + (1|year) crossed random effects (group and year)    
     
Pick error distribution with family: binomial, gaussian, poisson, Gamma or nbinom2, beta_family(), betabinomial ...   
Can use function name for base or list for others like family=list(family="nbinom2", link="log")      
Can select a zero-inflaction model with the ziformula argument   
Can select a dispersion model with fixed effects   
```{r}
# Brook et al. 2019 Conservation Biology used glmmTMB  
library(glmmTMB) # <https://cran.r-project.org/web/packages/glmmTMB/vignettes/glmmTMB.pdf> 

# repeated measures are used as random effect.  
# zero-inflated Poisson model with single zero-inflation paramenter applied to all observations = (ziformula~1). To exclude zero-inflation (is default but) ziformula~0 
# fit_zipoisson <- glmmTMB(NCalls~(FT+ArrivalTime)*SexParent+   # would work for number of indiviudals in class or age
#                            offset(log(BroodSize))+(1|Nest),   # the offset allows for use of Poisson response; log(BroodSize) may be to add the assumption that total number of calls is proportional to the brood size
#                          data=Owls,
#                          ziformula=~1,
#                          family=poisson)
```
Used in Brook et al. 2019 response variabile as a mixture of Bernoulli [0-1] probability of recovering a positive count, Poisson process for positive or negative trend in the count.   
Hypothesis: increased frequency of zeros through time comes from both declining abundance or declining effort (so not reported, not able to be seen and counted).      

    1. If more zeros over time are due to declining abundance, Poisson model would prouce a negative slope and zero-inflated model a positive slope - because abundance down (more zeros) but effort (as harvest here) would be increasing and compensate.       
    2. If more zeros are due to declining effort then both models would yeild negative slopes. 





<https://stats.idre.ucla.edu/r/dae/zip/>    
```{r}
library(ggplot2)
library(pscl)
library(boot)

# Example is measuring things correlated to how many fish caught but many people didn't catch fish, other people didn't fish at all, so zeros from both processes 
zinb <- read.csv("https://stats.idre.ucla.edu/stat/data/fish.csv")
zinb <- within(zinb, {
    nofish <- factor(nofish)
    livebait <- factor(livebait)
    camper <- factor(camper)
})

# histogram with x axis in log10 scale
ggplot(zinb, aes(count))+
  geom_histogram()+
  scale_x_log10()+
  theme_bw()

```

Ways to deal with zero inflation   
       1. Zero-inflated negative binomial regression: does better with over dispersed data (vaiance much larger than the mean)      
       2. Ordinary count: Poisson or negative binomial if there are no excess zeros      
       3. OLS regression: but not good when non-normal distribution     
       4. Zero-inflated Poisson    

```{r}
summary(m1 <- zeroinfl(count ~ child + camper | persons, data = zinb))
# Count model are the Poisson regression coefficients 
# Inflation model with logi coefficients for predicting excess zeros
# the p-values say it fits better than the null model

```

to determine if the zero-inflated model is an improvement over standard Poisson regression, compare with Vuong test
```{r}
summary(p1 <- glm(count ~ child + camper, family = poisson, data = zinb))

vuong(p1, m1) # compares the zero-inflated model with ordinary Poisson regression model. Vuong test for non-nested models complaints. Should just do a real AIC or maybe just like other AIC the non-nested comparisons are fine

# Confidence intervals for parameters and exponentiated parameters using bootstrapping, use 'boot' package. incidne trisk rations for Poisson model, odds ratio for zero inflation model. 
```


























Verision 1: pre 2019-12-01 December 

Model uncertainty
1. pick a threshold  
2. model the impact of spatial uncertanty across a landscape where there is varying environemntal differences and the scale of landscape variables is near reality vs. the spatial range of rare species
3. various levels of autocorrelated environmental variables

```{r}

GS <- runif(100, min=100, max = 50000)
B1 <- 10000 #intercept
B2 <- -0.6 #slope
Uncertainty <- (B1 + GS*B2)
UnJit <- Uncertainty*rnorm(100, mean = .5, sd = .15)

plot(GS,jitter(Uncertainty,amount = 0.0005), xlab="Geographic distribution", ylab= "Uncertainty")
plot(GS, (UnJit+abs(min(UnJit)))/max((UnJit+abs(min(UnJit)))), xlab="Geographic distribution", ylab= "Uncertainty")
```

Triangle plot for rarity
```{r}
library(ade4)
```


```{r}
#Environmental space is correlated to range
Environmental_Specificity <- jitter(2 + GS*0.91, amount = 100)

# local abundance low, more likely to miss if in clumps so if more environemtal habitat specificity, then shoudl be more lower samples, less detection (Dennett 2018)
Local_Abundance <- jitter(1 + ES*0.1, amount = 100)
Geographic_Range <- GS

raretaxa <- data.frame(Geographic_Range, Environmental_Specificity, Local_Abundance)

triangle.plot(raretaxa) # label = c("Geographic range","Habitat specificity","Local Abundance"))

```








